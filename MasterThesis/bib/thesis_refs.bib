% This file was created with Citavi 6.8.0.0

@misc{Alman.12.10.2020,
	abstract = {The complexity of matrix multiplication is measured in terms of {\$}$\backslash$omega{\$}, the smallest real number such that two {\$}n$\backslash$times n{\$} matrices can be multiplied using {\$}O(n{\^{}}{\omega+\epsilon}){\$} field operations for all {\$}$\backslash$epsilon{\textgreater}0{\$}; the best bound until now is {\$}$\backslash$omega{\textless}2.37287{\$} [Le Gall'14]. All bounds on {\$}$\backslash$omega{\$} since 1986 have been obtained using the so-called laser method, a way to lower-bound the `value' of a tensor in designing matrix multiplication algorithms. The main result of this paper is a refinement of the laser method that improves the resulting value bound for most sufficiently large tensors. Thus, even before computing any specific values, it is clear that we achieve an improved bound on {\$}$\backslash$omega{\$}, and we indeed obtain the best bound on {\$}$\backslash$omega{\$} to date: {\$}{\$}$\backslash$omega {\textless} 2.37286.{\$}{\$} The improvement is of the same magnitude as the improvement that [Le Gall'14] obtained over the previous bound [Vassilevska W.'12]. Our improvement to the laser method is quite general, and we believe it will have further applications in arithmetic complexity.},
	author = {Alman, Josh and Williams, Virginia Vassilevska},
	date = {12.10.2020},
	title = {A Refined Laser Method and Faster Matrix Multiplication},
	url = {http://arxiv.org/pdf/2010.05846v1},
	file = {http://arxiv.org/abs/2010.05846v1},
	file = {https://arxiv.org/pdf/2010.05846v1.pdf}
}


@article{Germain.,
	abstract = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.},
	author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
	title = {MADE: Masked Autoencoder for Distribution Estimation},
	url = {http://arxiv.org/pdf/1502.03509v2},
	file = {d0ad824e-1180-45ff-95c7-85c7624c6c31:C\:\\Users\\Luka\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\y6jrdgmy9x32qyy9dye4ewdw5dbakc4974n0qdkoz6sagf\\Citavi Attachments\\d0ad824e-1180-45ff-95c7-85c7624c6c31.pdf:pdf;bee2495c-db21-4eac-bc62-3bb0fe73d0a2:C\:\\Users\\Luka\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\y6jrdgmy9x32qyy9dye4ewdw5dbakc4974n0qdkoz6sagf\\Citavi Attachments\\bee2495c-db21-4eac-bc62-3bb0fe73d0a2.pdf:pdf;53cc6bfe-0da0-4361-8a14-ea06561c9321:C\:\\Users\\Luka\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\y6jrdgmy9x32qyy9dye4ewdw5dbakc4974n0qdkoz6sagf\\Remote Attachments\\53cc6bfe-0da0-4361-8a14-ea06561c9321.pdf:pdf}
}


@article{Kobyzev.2020,
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
	year = {2020},
	title = {Normalizing Flows: An Introduction and Review of Current Methods},
	url = {http://arxiv.org/pdf/1908.09257v4},
	pages = {1},
	issn = {0162-8828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	doi = {10.1109/TPAMI.2020.2992934},
	file = {df3be596-bf15-4412-a768-6eab1e8115a0:C\:\\Users\\Luka\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\y6jrdgmy9x32qyy9dye4ewdw5dbakc4974n0qdkoz6sagf\\Remote Attachments\\df3be596-bf15-4412-a768-6eab1e8115a0.pdf:pdf;1da9371b-747e-4a05-9870-b5b1a8b46053:C\:\\Users\\Luka\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\y6jrdgmy9x32qyy9dye4ewdw5dbakc4974n0qdkoz6sagf\\Citavi Attachments\\1da9371b-747e-4a05-9870-b5b1a8b46053.pdf:pdf}
}


@book{McLachlan.1988,
	author = {McLachlan, Geoffrey J. and Basford, Kaye E.},
	year = {1988},
	title = {Mixture models: Inference and applications to clustering},
	address = {New York},
	publisher = {{Marcel Dekker}},
	isbn = {0824776917}
}


@book{Milnor.1997,
	author = {Milnor, John W.},
	year = {1997},
	title = {Topology from the differentiable viewpoint},
	keywords = {Differential topology},
	address = {Princeton N.J.},
	edition = {Rev. ed.},
	publisher = {{Princeton University Press}},
	isbn = {0691048339},
	series = {Princeton landmarks in mathematics},
	file = {http://www.loc.gov/catdir/description/prin051/97030986.html},
	file = {http://www.loc.gov/catdir/toc/prin051/97030986.html}
}


@article{Nachman.2020,
	author = {Nachman, Benjamin and Shih, David},
	year = {2020},
	title = {Anomaly detection with density estimation},
	volume = {101},
	number = {7},
	issn = {2470-0010},
	journal = {Physical Review D},
	doi = {10.1103/PhysRevD.101.075042},
	file = {73b538a2-6d1d-4048-a37b-b51ff9eb5a15:C\:\\Users\\Luka\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\y6jrdgmy9x32qyy9dye4ewdw5dbakc4974n0qdkoz6sagf\\Citavi Attachments\\73b538a2-6d1d-4048-a37b-b51ff9eb5a15.pdf:pdf}
}


@article{Papamakarios.2021,
	abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	year = {2021},
	title = {Normalizing Flows for Probabilistic Modeling and Inference},
	url = {http://arxiv.org/pdf/1912.02762v2},
	volume = {22},
	journal = {Journal of Machine Learning Research},
	file = {9e80bb91-87f5-4012-bb19-1722a1c75c89:C\:\\Users\\Luka\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\y6jrdgmy9x32qyy9dye4ewdw5dbakc4974n0qdkoz6sagf\\Citavi Attachments\\9e80bb91-87f5-4012-bb19-1722a1c75c89.pdf:pdf;3652d7e1-8fdb-42a1-885c-59527e6bcdde:C\:\\Users\\Luka\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\y6jrdgmy9x32qyy9dye4ewdw5dbakc4974n0qdkoz6sagf\\Citavi Attachments\\3652d7e1-8fdb-42a1-885c-59527e6bcdde.pdf:pdf;c2f58370-f001-4eb8-b393-85e7ce25fa94:C\:\\Users\\Luka\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\y6jrdgmy9x32qyy9dye4ewdw5dbakc4974n0qdkoz6sagf\\Remote Attachments\\c2f58370-f001-4eb8-b393-85e7ce25fa94.pdf:pdf}
}


@misc{Papamakarios.19.05.2017,
	abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	date = {19.05.2017},
	title = {Masked Autoregressive Flow for Density Estimation},
	url = {http://arxiv.org/pdf/1705.07057v4},
	file = {http://arxiv.org/abs/1705.07057v4},
	file = {9fb72e23-b821-4d81-846b-5ec505bed15f:C\:\\Users\\Luka\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\y6jrdgmy9x32qyy9dye4ewdw5dbakc4974n0qdkoz6sagf\\Citavi Attachments\\9fb72e23-b821-4d81-846b-5ec505bed15f.pdf:pdf;fe0011ce-3653-42c2-b853-e2ae710c08a9:C\:\\Users\\Luka\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\y6jrdgmy9x32qyy9dye4ewdw5dbakc4974n0qdkoz6sagf\\Remote Attachments\\fe0011ce-3653-42c2-b853-e2ae710c08a9.pdf:pdf}
}


@phdthesis{Papamakarios.2019,
	author = {Papamakarios, Georgios and Murray, Iain and Williams, Chris},
	year = {2019},
	title = {Neural density estimation and likelihood-free inference},
	url = {https://hdl.handle.net/1842/36394},
	address = {Great Britain},
	school = {{University of Edinburgh}},
	type = {Thesis (Ph.D.)}
}


@article{Pedregosa.2011,
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	title = {Scikit-learn: Machine Learning in Python},
	pages = {2825--2830},
	volume = {12},
	journal = {Journal of Machine Learning Research}
}


